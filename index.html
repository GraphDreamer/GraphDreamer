<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="GraphDreamer, a novel approach to generating compositional 3D scenes that are grounded in scene graphs.">
  <meta name="keywords" content="GraphDreamer, scene graph grounding, GT3D">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>GraphDreamer: Compositional 3D Scene Synthesis from Scene Graphs</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <!-- <script async src="https://www.googletagmanager.com/gtag/js?id="></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', '');
  </script> -->

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://github.com/GGGHSL">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>
    </div>

  </div>
</nav>

<!-- Author + Links -->
<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <!-- Logo+Title -->
          <div class="myHeading">
            <img src="./static/images/favicon.svg" />
            <h1 class="title is-1 publication-title">GraphDreamer: Compositional 3D Scene Synthesis from Scene Graphs</h1>
          </div>
          <!-- /Logo+Title -->
          
          <!-- Authors -->
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://ggghsl.github.io">Gege Gao</a><sup>1,2,3,4</sup>,</span>
            <span class="author-block">
              <a href="https://wyliu.com">Weiyang Liu</a><sup>1,5</sup>,</span>
            <span class="author-block">
              <a href="https://apchenstu.github.io">Anpei Chen</a><sup>2,3,4</sup>,
            </span>
            <span class="author-block">
              <a href="https://www.cvlibs.net">Andreas Geiger</a><sup>3,4</sup>,
            </span>
            <span class="author-block">
              <a href="https://is.mpg.de/~bs">Bernhard Schölkopf</a><sup>1,2,4</sup>,
            </span>
          </div>
          <!-- /Authors -->
          <!-- Institutes -->
          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Max Planck Institute for Intelligent Systems - Tübingen,</span>
            <span class="author-block"><sup>2</sup>ETH Zürich,</span>
            <span class="author-block"><sup>3</sup>University of Tübingen,</span>
            <span class="author-block"><sup>4</sup>Tübingen AI Center,</span>
            <span class="author-block"><sup>5</sup>University of Cambridge</span>
            <p>
              <span style="font-weight: bold;">CVPR 2024</span>
            </p>
          </div>
          <!-- /Institutes -->

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2312.00093.pdf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <!-- arXiv Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/abs/2312.00093"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Video Link. -->
              <!-- <span class="link-block">
                <a href="https://www.youtube.com/watch?v="
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span> -->
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/GGGHSL/GraphDreamer"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span title="Coming soon!">Code</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Teaser -->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <div class="content has-text-centered">
        <!-- <embed src="./static/images/teaser.pdf" width="1047px" height="auto"> -->
        <img src="./static/images/teaser.jpg"
          class="interpolation-image"
          alt="teaser image."/>
      </div>
      <h2 class="subtitle has-text-centered">
        <b>GraphDreamer</b> takes scene graphs as input and generates object compositional 3D scenes.
      </h2>
    </div>
  </div>
</section>


<!-- Videos Figure3 -->
<section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">        
        
        <div class="item item-figure3-row1">
          <video poster="" id="figure3-row1" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/figure3_row1.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-figure3-row2">
          <video poster="" id="figure3-row2" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/figure3_row2.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-figure3-row3">
          <video poster="" id="figure3-row3" autoplay controls muted loop playsinline width="100%">
            <source src="./static/videos/figure3_row3_v2.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-figure3-row4">
          <video poster="" id="figure3-row4" autoplay controls muted loop playsinline width="100%">
            <source src="./static/videos/figure3_row4_v2.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-figure3-row5">
          <video poster="" id="figure3-row5" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/figure3_row5.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-figure3-row6">
          <video poster="" id="figure3-row6" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/figure3_row6.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-figure3-row7">
          <video poster="" id="figure3-row7" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/figure3_row7.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-figure3-row8">
          <video poster="" id="figure3-row8" autoplay controls muted loop playsinline width="100%">
            <source src="./static/videos/figure3_row8_v2.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Abstract + Video + Pipeline -->
<section class="section">
  <div class="container is-max-desktop">
    
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            We present <i>GraphDreamer</i>, the first framework capable of generating compositional 3D scenes from scene graphs, where objects are represented as nodes and their interactions as edges. 
          </p>
          <p>
            As pretrained text-to-image diffusion models become increasingly powerful, recent efforts have been made to distill knowledge from these text-to-image pretrained models for optimizing a text-guided 3D model. Most of the existing methods generate a holistic 3D model from a plain text input. This can be problematic when the text describes a complex scene with multiple objects, because the vectorized text embeddings are inherently unable to capture a complex description with multiple entities and relationships. Holistic 3D modeling of the entire scene further prevents accurate grounding of text entities and concepts. 
          </p>
          <p>
            Our method makes better use of the pretrained text-to-image diffusion model by exploiting node and edge information in scene graphs, and is able to fully disentangle different objects without image-level supervision. To facilitate modeling of object-wise relationships, we use signed distance fields as representation and impose a constraint to avoid inter-penetration of objects. To avoid manual scene graph creation, we design a text prompt for ChatGPT to generate scene graphs based on text inputs. We conduct both qualitative and quantitative experiments to validate the effectiveness of GraphDreamer in generating high-fidelity compositional 3D scenes with disentangled object entities.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

    <!-- Paper video. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <!-- <h2 class="title is-3">Video</h2> -->
        <div class="publication-video">
          <iframe src="https://www.youtube.com/watch?v=vDhhf6BA1E4&t=6s"
                  frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
        </div>
      </div>
    </div>
    <!--/ Paper video. -->

    <!-- Pipeline. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Method</h2>
        <!-- <div class="content has-text-centered">
          <embed src="./static/images/paper_pipeline.pdf" width="404px" height="auto">
        </div> -->
        <!-- <embed src="./static/images/paper_pipeline.pdf" width="404px" height="auto"> -->
        <img src="./static/images/paper_pipeline.jpg"
          class="interpolation-image"
          alt="paper pipeline."/>
        
        <div class="content has-text-justified">
          <p>GraphDreamer decomposes the scene graph into global, node-wise and edge-wise text description, and represents and optimizes the SDF-based objects individually with identity-aware object fields.</p>
        </div>
      </div>
    </div>
    <!--/ Pipeline. -->
  </div>
</section>


<!-- Videos Figure5 -->
<section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <h2 class="title is-3 has-text-centered">Leveraging Scene Graphs For Text Grounding</h2>
      <!-- Demo. -->
      <video poster="" id="demo" autoplay controls muted loop playsinline height="100%">
        <source src="./static/videos/demo.mp4"
                type="video/mp4">
      </video>
      <!--/ Demo. -->
      <div class="content has-text-justified">
        <p>To save the effort of building a scene graph from scratch, the scene graph can be generated by a language model (e.g., ChatGPT) from a user text input.</p>
      </div>
    </div>
    
    <div class="container is-max-desktop">
      <!-- <div id="widget-carousel" class="carousel widget-carousel"> -->
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-figure5-1">
          <video poster="" id="figure5-1" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/figure5_1.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-figure5-2">
          <video poster="" id="figure5-2" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/figure5_2.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-figure5-3">
          <video poster="" id="figure5-3" autoplay controls muted loop playsinline height="100%">
          <!-- width="100%" -->
            <source src="./static/videos/figure5_3.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-figure5-4">
          <video poster="" id="figure5-4" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/figure5_4.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-figure5-5">
          <video poster="" id="figure5-5" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/figure5_5_v2.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-figure5-6">
          <video poster="" id="figure5-6" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/figure5_6.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Inverse Semantics. -->
<section class="section">
  <div class="container is-max-desktop">
    <!-- <h2 class="title is-3">Inverse Semantics</h2>
    <p>
      We present a new paradigm of semantic reconstruction with GPT4V-guided GraphDreamer.
    </p> -->

    <!-- Inverse. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3 has-text-centered">Inverse Semantics: A New Paradigm</h2>
        <h2 class="subtitle has-text-justified">
          We also present a new paradigm of semantic reconstruction with GPT4V-guided GraphDreamer.
        </h2><br/>

        <!-- 3 Nodes. -->
        <h3 class="title is-4">Extract scene graph from images</h3>
        <div class="content has-text-justified">
          <p>
            Using GraphDreamer, we can inverse the semantics in a given image into a 3D scene, by extracting a scene graph directly from an input image with GPT4V, restricting the nodes present to the most salient ones.
          </p>
        </div>
        <div class="content has-text-centered">
          <!-- <iframe id="inverse-3n" src="./static/images/inverse_3n.pdf" type="application/pdf" height="100%" width="100%">
          </iframe> -->
          <img src="./static/images/inverse_3n.jpg"
               class="interpolation-image"
               alt="Interpolate inverse example image 1."/>
        </div>

        <!-- <div class="columns is-vcentered interpolation-panel">
          <div class="column is-3 has-text-centered">
            <img src="./static/images/interpolate_start.jpg"
                 class="interpolation-image"
                 alt="Interpolate start reference image."/>
            <p>Start Frame</p>
          </div>
          <div class="column interpolation-video-column">
            <div id="interpolation-image-wrapper">
              Loading...
            </div>
            <input class="slider is-fullwidth is-large is-info"
                   id="interpolation-slider"
                   step="1" min="0" max="100" value="0" type="range">
          </div>
          <div class="column is-3 has-text-centered">
            <img src="./static/images/interpolate_end.jpg"
                 class="interpolation-image"
                 alt="Interpolation end reference image."/>
            <p class="is-bold">End Frame</p>
          </div>
        </div> -->
        <br/>
        <!--/ 3 Nodes. -->

        <!-- 5 Nodes. -->
        <h3 class="title is-4">Identify object centers for initialization</h3>
        <div class="content has-text-justified">
          <p>
            To inverse more complex semantics with more salient nodes in an image, we can ask GPT to provide with center coordinates for each object and initialize the SDF-based objects as spheres centered at these coordinates.
          </p>
        </div>
        <div class="content has-text-centered">
          <!-- <video id="replay-video"
                 controls
                 muted
                 preload
                 playsinline
                 width="75%">
            <source src="./static/videos/replay.mp4"
                    type="video/mp4">
          </video> -->
          <!-- <iframe id="inverse-5n" src="./static/images/inverse_5n.pdf" type="application/pdf" height="100%" width="100%">
          </iframe> -->
          <img src="./static/images/inverse_5n.jpg"
               class="interpolation-image"
               alt="Interpolate inverse example image 2."/>
        </div>
        <!--/ 5 Nodes. -->

      </div>
    </div>
    <!--/ Inverse. -->

  </div>
</section>
<!--/ Inverse Semantics. -->

<section class="section" id="Acknowledgment">
  <div class="container is-max-desktop content has-text-justified">
    <h2 class="title">Acknowledgment</h2>
    <p>
      The authors extend their thanks to Zehao Yu and Stefano Esposito for their invaluable feedback on the initial draft. Our thanks also go to Yao Feng, Zhen Liu, Zeju Qiu, Yandong Wen, and Yuliang Xiu for their proofreading of the final draft and for their insightful suggestions which enhanced the quality of this paper. Additionally, we appreciate the assistance of those who participated in our user study. 
      Weiyang Liu and Bernhard Schölkopf was supported by the German Federal Ministry of Education and Research (BMBF): Tübingen AI Center, FKZ: 01IS18039B, and by the Machine Learning Cluster of Excellence, the German Research Foundation (DFG): SFB 1233, Robust Vision: Inference Principles and Neural Mechanisms, TP XX, project number: 276693517. Andreas Geiger and Anpei Chen were supported by the ERC Starting Grant LEGO-3D (850533) and the DFG EXC number 2064/1 - project number 390727645.
    </p>
  </div>
</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@Inproceedings{gao2024graphdreamer,
  author    = {Gao, Gege and Liu, Weiyang and Chen, Anpei and Geiger, Andreas and Schölkopf, Bernhard},
  title     = {GraphDreamer: Compositional 3D Scene Synthesis from Scene Graphs},
  booktitle = {Conference on Computer Vision and Pattern Recognition (CVPR)},
  year      = {2024},
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="https://arxiv.org/pdf/2312.00093.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/GGGHSL" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This means you are free to borrow the <a
              href="https://github.com/nerfies/nerfies.github.io">source code</a> of this website,
            we just ask that you link back to this page in the footer.
            Please remember to remove the analytics code included in the header of the website which
            you do not want on your website.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
